`./run_tests.py`:
```py
import os
from dotenv import load_dotenv
from src.unit_testing.testing import test_temperature_runs

# Load the .env file into the environment
load_dotenv()

# Set defaults or get from environment
QUESTION= os.getenv('QUESTION')
TEMP_START_VALUE = float(os.getenv('TEMP_START_VALUE'))
TEMP_RANGE = int(os.getenv('TEMP_RANGE'))
SAVE_DIR = os.getenv('SAVE_DIR') if os.getenv('SAVE_DIR') is not None else str("src/unit_testing/unit_test_results")
REPEAT = int(os.getenv('REPEAT'))

def main():
    # Run the tests
    test_temperature_runs(
        question=QUESTION,
        temp_start_value=TEMP_START_VALUE,
        temp_range=TEMP_RANGE,
        save_dir=SAVE_DIR,
        repeat=REPEAT
    )

if __name__ == '__main__':
    main()
```


`./main.py`:
```py
import os
import json

from typing import Optional
from termcolor import colored

from src.utils.logging import Logger
from src.content_generator import ContentGenerator
from src.utils.helpers import format_string
from src.utils.env_setup import EnvironmentSetup
from src.utils.file_handler import *

env_and_flags = EnvironmentSetup()

def main(
    model: Optional[str] = "gpt-3.5-turbo-16k",
    temperature: Optional[float] = 0.33,
    query_mode: Optional[bool] = False,
    openai_api_key: Optional[str]= None,
    super_charged: Optional[bool]=False,
    prompt_dir: Optional[str]=os.path.join(os.getcwd(), "resources", "prompts")
):
    print("Starting...")

    run_num = 0
    
    if temperature != None:
        print(colored(f"TEMPERATURE CHOSEN: {temperature}\n", 'red'))
    else:
        print(colored(f"DEFAULT TEMPERATURE: 0.33\n", 'red'))
    
    if model != None:
        print(colored(f"MODEL CHOSEN: {model}\n", 'red'))
    else:
        print(colored(f"DEFAULT MODEL USED: gpt-3.5-turbo-16k\n", 'red'))

    # Create a Logger and Initialize Env Vars
    logger_instance = Logger()
    logger = logger_instance.get_logger()

    logger.info(f"MODEL: {model}")
    censored_key = format_string(env_and_flags.openai_api_key)
    logger.info(f"OPENAI_API_KEY: {censored_key}")

    # Initialize a content generator
    content_generator = ContentGenerator(env_and_flags.model, env_and_flags.openai_api_key, env_and_flags.super_charged, env_and_flags.temperature)

    # Get user's question
    prompt_message = "Enter your question in one to two sentences. Try to make it as accurate, concise, and salient as possible:\n"
    colored_prompt = colored(prompt_message, 'green')
    main_user_question = input(colored_prompt + "\n" + colored("ENTER QUESTION HERE: ", 'red', attrs=['bold']))
    logger.info("The pre-formatted message you wrote:\n" + main_user_question)

    # Get final output, and save it for interactive querying
    final_output = content_generator.compile(main_user_question)
    saved_combined_content = [{"role": "system", "content": final_output[0]}, {"role": "user", "content": main_user_question}, {"role": "assistant", "content": final_output[1]}]

    # Assuming prompt_dir and run_num are defined
    run_num = FileManager.get_highest_run_num(prompt_dir) + 1
    FileManager.save_content(prompt_dir, 'prompt_and_answer', run_num, saved_combined_content, addition=False)
    
    def query_mode_caller(content_generator: ContentGenerator, prompt_dir: str, run_num: int) -> [str, bool]:
        if env_and_flags.query_mode == True:
            next_question = input(colored("ENTER NEXT QUERY HERE (or type exit to quit): ", 'red', attrs=['bold']))
            if next_question.lower() != "exit":
                latest_file_name = f"{prompt_dir}/prompt_and_answer_{run_num}.txt"  # Assuming the files are 0-indexed
                
                with open(latest_file_name, 'r') as file:
                    # Load the current conversation
                    current_dict = json.load(file)
                    
                    # Generate a completion with this new, full context
                    next_answer = content_generator.generate_plain_completion(current_dict, next_question)

                    # Get the next answer && append to current conversati
                    
                    # Return output
                    print(colored("Answer to your next question:\n\n" + next_answer, 'magenta'))
                    current_dict.append({"role": "user", "content": next_question})
                    current_dict.append({"role": "assistant", "content": next_answer})
                    FileManager.save_content(prompt_dir, file, run_num, current_dict, addition=True)
                    
                    # Return the message!
                    print(colored(next_answer, 'magenta'))
                    logger.info("Generated next answer:\n" + next_answer)
                    run_num += 1
                    return [next_answer, True]
            else:
                env_and_flags.query_mode=False
                print("Program completed.")
                return ["Program completed.", False]
        else:
            exit(0)

    [return_value, user_end_or_continue] = query_mode_caller(content_generator, prompt_dir, run_num)
    while user_end_or_continue:
        [return_value, user_end_or_continue] = query_mode_caller(content_generator, prompt_dir, run_num)
    if len(final_output) > 1:
        print(colored("Success! Please evaluate your results against a trusted source.", 'green'))
    else:
        logger.log(logger.error, ("Error occurred while parsing final output, perhaps index-related, perhaps incorrect function call or possibly context was too long."))
        exit(1)


if __name__ == "__main__":
    main(
        model=env_and_flags.model,
        temperature=env_and_flags.temperature,
        query_mode=env_and_flags.query_mode,
        openai_api_key=env_and_flags.openai_api_key,
        super_charged=env_and_flags.super_charged,
        prompt_dir=env_and_flags.prompt_dir
    )```


`./src/chat_completion_generator.py`:
```py
from __future__ import annotations

# Import essentials
import openai

# Import helpers, constants and typing
from termcolor import colored
from typing import Optional
from typing import List
from src.utils.helpers import sum_content_length
from src.utils.logging import Logger
from src.utils.env_setup import EnvironmentSetup


log_obj = Logger()
logger = log_obj.get_logger()
env_and_flags = EnvironmentSetup()

class ChatCompletionGenerator:
    def __init__(self, temperature: Optional[float]=env_and_flags.temperature, prompt_num: Optional[int] = 0, openai_api_key: Optional[str] = env_and_flags.openai_api_key, model: Optional[str] = env_and_flags.model, super_charged: Optional[str] = env_and_flags.super_charged):
        """
        Constructor for the SystemMessageMaker class.
        
        Args:
            prompt_num (int, optional): The number of the prompt to use. Defaults to None.
            openai_api_key (str, optional): The API key for OpenAI. Defaults to None.
            model (str, optional): The model for OpenAI. Defaults to None.
            super_charged (str, optional): The super charged mode for GPT-4. Defaults to None.
        """
        self.prompt_num = prompt_num
        self.openai_api_key = env_and_flags.openai_api_key
        self.model = env_and_flags.model
        self.super_charged = env_and_flags.super_charged
        openai.api_key = self.openai_api_key
        self.temperature = env_and_flags.temperature

    def generate_completion(self, model, messages: List[dict], temperature: Optional[float]=0.33) -> str:
        logger.info(f"MODEL ACTUALLY BEING USED: {model}")
        """
        Generates a completion using OpenAI's ChatCompletion API.

        Args:
            messages (List[dict]): A list of messages to start the completion. Each message is a dictionary containing 'role' and 'content' keys.
            model (str, optional): The model to use for the completion. Defaults to "gpt-3.5-turbo".
            temperature (float, optional): The higher the number used, the more statistical variation / randomness. Between 0.1-0.5 is reocmmended for coding and facts.

        Returns:
            str: The content of the completion generated by the model.
        """
        print(colored("\nGenerating completion...\n", 'magenta'))
        print(colored(f"Current context length: {sum_content_length(messages)}\n", 'red'))

        response = openai.ChatCompletion.create(
            model=model,
            messages = messages,
            max_tokens = 4000,
            temperature = temperature
        )
        print(colored("--Successfully completed last API call--\n", 'blue'))
        return response['choices'][0]['message']['content']```


`./src/utils/logging.py`:
```py
from dotenv import load_dotenv
from typing import Optional
import os
import logging
class Logger:
    _instance = None  # Singleton instance

    def __new__(cls, *args, **kwargs):
        if not isinstance(cls._instance, cls):
            cls._instance = super(Logger, cls).__new__(cls, *args, **kwargs)
        return cls._instance

    def __init__(self, log_dir: Optional[str] = "logs/"):
        # This ensures that the initialization happens only once
        if not hasattr(self, 'initialized'):
            self.initialized = True

            # Set up logging
            os.makedirs(log_dir, exist_ok=True)
            log_file = os.path.join(log_dir, 'log.txt')
            logging.basicConfig(filename=log_file, 
                                level=logging.INFO, 
                                format='%(asctime)s %(levelname)s %(name)s %(message)s')
            self.logger = logging.getLogger(__name__)
            self.log_dir = log_dir

    def get_logger(self) -> logging.Logger:
        return self.logger```


`./src/utils/file_handler.py`:
```py
import os
import re
import json
from typing import Optional

class FileManager:
    
    @staticmethod
    def get_highest_run_num(directory):
        highest_run_num = 0
        for filename in os.listdir(directory):
            match = re.match(r'prompt_and_answer_(\d+)\.txt', filename)
            if match:
                run_num = int(match.group(1))
                highest_run_num = max(highest_run_num, run_num)
        return highest_run_num

    @staticmethod
    def ensure_directory_exists(directory):
        if not os.path.exists(directory):
            os.makedirs(directory)

    @staticmethod
    def get_next_available_filename(directory, base_filename, run_num, addition: Optional[bool] = False):
        if not addition:
            while True:
                filename = f"{directory}/prompt_and_answer_{run_num}.txt"
                if not os.path.exists(filename):
                    return filename
                run_num += 1
        else:
            return f"{directory}/prompt_and_answer_{run_num}.txt"
    
    @staticmethod
    def save_content(directory, base_filename, run_num, content, addition: Optional[bool] = False) -> None:
        FileManager.ensure_directory_exists(directory)
        filename = base_filename
        if not addition:
            filename = FileManager.get_next_available_filename(directory, base_filename, run_num, addition)
        else:
            filename = f"{directory}/prompt_and_answer_{run_num}.txt"
        with open(filename, 'w') as file:
            json.dump(content, file, indent=2)```


`./src/utils/env_setup.py`:
```py
import argparse
import os
from dotenv import load_dotenv

class EnvironmentSetup:
    _instance = None

    @staticmethod
    def parse_arguments():
        parser = argparse.ArgumentParser(description="An easier and far cheaper way to use the gpt-3.5-turbo API, with results nearly comparable to ChatGPT 4 - with more additions coming soon!")
        parser.add_argument("--model", type=str, help="Your chosen OpenAI model")
        parser.add_argument("--temperature", type=float, help="Higher value is more random/creative.")
        parser.add_argument("--query_mode", type=bool, help="Enter True or False to continue asking questions in this context.")
        parser.add_argument("--openai_api_key", type=str, help="Your OPENAI_API_KEY")
        parser.add_argument("--super_charged", type=lambda x: (str(x).lower() == 'true'), help="If you have access to GPT-4, empowers results.")
        parser.add_argument("--prompt_dir", type=str, help="The directory for which you'd like to save your prompts and answer histories.", default="resources/prompts")
        return parser.parse_args()

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(EnvironmentSetup, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        load_dotenv()
        self.args = self.parse_arguments()
        self.openai_api_key = self.args.openai_api_key if self.args.openai_api_key is not None else os.getenv("OPENAI_API_KEY")
        self.model = self.args.model or os.getenv("MODEL")
        self.super_charged = self.args.super_charged if self.args.super_charged is not None else (os.getenv("SUPER_CHARGED") == 'True')
        self.temperature = self.args.temperature or None
        self.query_mode = self.args.query_mode or None
        self.prompt_dir = self.args.prompt_dir or None
        
        self._initialized = True
```


`./src/utils/helpers.py`:
```py
from src.utils.constants import AUTOMATED_CONTEXT_CALLER
import json

def sum_content_length(messages: json):
    """
    Function to sum up the length of the content values in a list of messages.
    
    Args:
        messages (list of dict): List of message dictionaries.

    Returns:
        int: The total length of all content strings.
    """
    total_length = 0
    for message in messages:
        content = message.get('content', '')
        total_length += len(content)
        
    return total_length

def format_string(s: str) -> str:
    """Format a string to show the first 7 characters, last 4 characters, with stars in between.

    Args:
        s (str): The input string

    Returns:
        str: The formatted string

    Raises:
        ValueError: If the length of the string is 0
    """
    
    if len(s) == 0:
        raise ValueError("The input string must not be empty.")
    
    return s[:7] + '*' * 7 + s[-4:]

def compile_unit_test_prompts(directory='.'):
    from pathlib import Path
    # Using pathlib to work with files
    directory_path = Path(directory)
    
    # List all files that match the given pattern
    files = [f for f in directory_path.iterdir() if f.name.startswith('temperature_') and f.name.endswith('.txt')]
    
    # Sort the files based on the float value after the last underscore
    files.sort(key=lambda x: float(x.stem.rsplit('_', 1)[-1]))

    # Using a single file write operation to write all contents at once
    with (directory_path / 'compilation.txt').open('w') as output_file:
        # List comprehension to build the contents for the compilation
        contents = [f"{file.name}:\n{file.read_text()}\n\n" for file in files]
        output_file.write(''.join(contents))```


`./src/content_generator.py`:
```py
from typing import Optional, Tuple
from src.utils.constants import FILE_FORMATTER, QUESTION_FIXER_PART_ONE, QUESTION_FIXER_PART_TWO,\
    CONTEXT_EXPANSION, TREE_OF_THOUGHT_MAKER_SECOND_HALF, TREE_OF_THOUGHT_MAKER_FIRST_HALF,\
        AUTOMATED_CONTEXT_CALLER
from src.chat_completion_generator import ChatCompletionGenerator
from src.utils.logging import Logger
from termcolor import colored
from src.utils.env_setup import EnvironmentSetup

log_obj = Logger()
logger = log_obj.get_logger()
env_and_flags = EnvironmentSetup()

class ContentGenerator:
    def __init__(self, prompt_num: Optional[int] = None, model: Optional[str] = None, super_charged: Optional[str] = None, default_compilation: Optional[str] = "", temperature: Optional[float] = 0.33):
        """
        Constructor for the ContentGenerator class
        
        Args:
            prompt_num (int, optional): The number of the prompt to use. Defaults to None.
            openai_api_key (str, optional): The API key for OpenAI. Defaults to None.
            model (str, optional): The model for OpenAI. Defaults to None.
            super_charged (str, optional): The super charged mode for GPT-4. Defaults to None.
            default_completion (str, optional): Defaults to nothing, but if you want a chat history, input what you'd like for querying purposes.
        """
        # Basics
        self.context_total = ""
        self.prompt_num = prompt_num
        self.openai_api_key = env_and_flags.openai_api_key
        self.model = env_and_flags.model
        self.super_charged = env_and_flags.super_charged
        self.temperature = temperature
        
        # Initialize a gpt-3.5-turbo chat completer
        self.chat_completer_big = ChatCompletionGenerator(temperature=temperature, prompt_num=prompt_num, openai_api_key=self.openai_api_key, model="gpt-4-0314", super_charged=self.super_charged)
        
        # Initialize a gpt-4-0314 chat completer (more powerful)
        self.chat_completer_small = ChatCompletionGenerator(prompt_num=prompt_num, openai_api_key=self.openai_api_key, model="gpt-3.5-turbo-16k", super_charged=self.super_charged, temperature=self.temperature)

    from typing import List, Dict

    def generate_plain_completion(self, json_file_input: List[Dict[str, str]], query: str):
        # Ensure json_file_input is not empty and is a list
        if not json_file_input or not isinstance(json_file_input, list):
            print(colored("INVALID CONVERSATION HISTORY", 'red'))
            return None

        # Get the last system and user messages from the conversation history
        last_system_message = json_file_input[-2]['content'] if len(json_file_input) > 1 else ""
        last_user_message = json_file_input[-1]['content'] if json_file_input else ""

        # Combine the last system message, last user message, and new user query into one string
        full_input_for_completion = f"{last_system_message}\n{last_user_message}\n{query}"

        # Generate a new completion using the chat_completer_big
        new_completion = self.chat_completer_big.generate_completion(env_and_flags.model, [
            {"role": "system", "content": last_system_message},
            {"role": "user", "content": last_user_message + "\n" + query}
        ])

        return new_completion

    def perfect_question(self, user_input_question: str):
        total_fixer_prompt = QUESTION_FIXER_PART_ONE + user_input_question + QUESTION_FIXER_PART_TWO
        print(colored(total_fixer_prompt, 'yellow'))
        fixed_user_input_question = self.chat_completer_big.generate_completion(env_and_flags.model, [{"role": "system", "content": total_fixer_prompt}])
        return fixed_user_input_question

    def make_initial_context(self, user_input_question: str):
        if user_input_question == "":
            print(colored("NO QUESTION PROVIDED", 'red'))
            exit(1)
        full_input_for_context = AUTOMATED_CONTEXT_CALLER + "\n\n" + user_input_question
        print(colored(full_input_for_context, 'yellow'))
        initial_context = self.chat_completer_big.generate_completion(env_and_flags.model, [{"role": "system", "content": AUTOMATED_CONTEXT_CALLER}, {"role": "user", "content": user_input_question}])
        return initial_context

    def expand_context(self, initial_context: str):
        expanded_context = self.chat_completer_big.generate_completion(env_and_flags.model, [{"role": "system", "content": CONTEXT_EXPANSION}, {"role": "user", "content": initial_context}])
        return expanded_context

    def make_tree_of_thought_final(self, expanded_context: str):
        total_system_message_input = TREE_OF_THOUGHT_MAKER_FIRST_HALF + "[CONTEXT]:\n" + expanded_context + "\n\n" + TREE_OF_THOUGHT_MAKER_SECOND_HALF
        tree_of_thought_final = self.chat_completer_big.generate_completion(env_and_flags.model, [{"role": "system", "content": total_system_message_input}])
        print(colored("Final Tree of Thought Prompt:\n", 'magenta'))
        print(colored(tree_of_thought_final + "\n", 'green'))
        logger.info(f"Tree Of Thought Generated:\n")
        logger.info(f"{tree_of_thought_final}")
        return tree_of_thought_final

    def get_final_answer(self, tree_of_thought_final: str, user_question: str):
        updated_user_question = "Experts, please come to a consensus on the following question:\n\n" + user_question
        print(colored("Final question to be asked:\n", 'red'))
        print(colored(updated_user_question, 'magenta'))
        final_answer = self.chat_completer_big.generate_completion(env_and_flags.model, [{"role": "system", "content": tree_of_thought_final}, {"role": "user", "content": updated_user_question}])
        print(colored("Generated answer:\n", 'magenta'))
        print(colored(final_answer, 'magenta'))
        logger.info(f"Tree Of Thought Generated:\n")
        logger.info(f"{final_answer}")
        return final_answer

    def compile(self, user_input_question) -> Tuple:
        # Fix question
        perfected_question = self.perfect_question(user_input_question)
        print(colored("Your formatted question is below:\n", 'blue'))
        print(colored(perfected_question + "\n", 'green'))
        
        # Make initial list of related topics
        inital_context = self.make_initial_context(perfected_question)
        print(colored("Initial context:\n", 'red'))
        print(colored(inital_context, 'magenta'))
        
        # Expand on that substantially
        expanded_context = self.expand_context(inital_context)
        print(colored("Expanded context:", 'red'))
        print(colored(expanded_context, 'magenta'))

        # Use Tree of Thought to increase the robustness of a response
        tree_of_thought_final = self.make_tree_of_thought_final(expanded_context)
        
        # Retrieve a final answer
        final_answer = self.get_final_answer(tree_of_thought_final, perfected_question)
        return [tree_of_thought_final, final_answer]
    
    def format_file_name(self, user_input_question: str) -> str:
        formatted_file_name = self.chat_completer_big.generate_completion(env_and_flags.model, [{"role": "system", "content": FILE_FORMATTER}, {"role": "user", "content": user_input_question}])
        return formatted_file_name```


